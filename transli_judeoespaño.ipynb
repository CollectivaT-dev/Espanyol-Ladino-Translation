{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transli_judeoespaño.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install stanza"
      ],
      "metadata": {
        "id": "aiD3ZSoK14Ss"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "EvwiKcTY1EXv"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import stanza\n",
        "\n",
        "stanza.download('es')\n",
        "nlp = stanza.Pipeline('es')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file1 = open('dic-judeo-español.txt', 'r', encoding=\"utf-8\")\n",
        "Lines = file1.readlines()\n",
        "dic=[]\n",
        "for l in Lines:\n",
        "  p = {\"src\":l.split(\";\")[0],\"target\":l.split(\";\")[1]}\n",
        "  dic.append(p)"
      ],
      "metadata": {
        "id": "sWNRbONS-r1F"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "def remove_punctuation (s):\n",
        "  return re.sub('[%s]' % re.escape(string.punctuation), ' ', s)"
      ],
      "metadata": {
        "id": "Wo_0mnvUCryK"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def judeo_parse(s):\n",
        "  if s.find(\"h\") != -1:\n",
        "    indices = [i for i, x in enumerate(s) if x == \"h\"]\n",
        "    flag_h = 0\n",
        "    for h in indices:\n",
        "      string_list = list(s)\n",
        "      if h != 0:\n",
        "        if s[h-2] != \"c\":\n",
        "          string_list[h] = \"\"\n",
        "      else:\n",
        "        string_list[0] = \"\"\n",
        "    s = \"\".join(string_list)\n",
        "  if s.find(\"y\") != -1:\n",
        "    if len(s) == 1:\n",
        "      s = s.replace(\"y\",\"i\")\n",
        "  if s.find(\"ca\") != -1:\n",
        "    s = s.replace(\"ca\",\"ka\")\n",
        "  if s.find(\"co\") != -1:\n",
        "    s = s.replace(\"co\",\"ko\")\n",
        "  if s.find(\"ñ\") != -1:\n",
        "    s = s.replace(\"ñ\",\"ny\")\n",
        "  if s.find(\"qu\") != -1:\n",
        "    s = s.replace(\"qu\",\"k\")\n",
        "  if s.find(\"rd\") != -1:\n",
        "    s = s.replace(\"rd\",\"dr\")\n",
        "  if s.find(\"cr\") != -1:\n",
        "    s = s.replace(\"cr\",\"kr\")\n",
        "  if s.find(\"ll\") != -1:\n",
        "    s = s.replace(\"ll\",\"y\")\n",
        "  if s.find(\"g\") != -1:\n",
        "    s = s.replace(\"g\",\"j\")\n",
        "  return s"
      ],
      "metadata": {
        "id": "V7sassJ_DCwt"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def complete_word(mayor,menor):\n",
        "  for l in range(len(mayor)-len(menor)):\n",
        "    menor = menor+\" \"\n",
        "  return menor\n",
        "\n",
        "def get_suffix(a,b):\n",
        "  flag = 0\n",
        "  mayor = \"\"\n",
        "  menor = \"\"\n",
        "\n",
        "  if len(a) > flag:\n",
        "    mayor = a\n",
        "    menor = b\n",
        "    flag = len(a)\n",
        "  if len(b) > flag:\n",
        "    mayor = b\n",
        "    menor = a\n",
        "  menor = complete_word(mayor,menor)\n",
        "\n",
        "  f = []\n",
        "  for x, y in zip(mayor,menor):\n",
        "    if x == y:\n",
        "      f.append(x)\n",
        "\n",
        "  h = ''.join([str(elem) for elem in f])\n",
        "\n",
        "  for l in range(len(a)-len(h)):\n",
        "    h = h+\" \"\n",
        "\n",
        "  p = []\n",
        "  for x, y in zip(a,h):\n",
        "    if x != y:\n",
        "      p.append(x)\n",
        "\n",
        "  p = ''.join([str(elem) for elem in p])\n",
        "  \n",
        "  return p"
      ],
      "metadata": {
        "id": "vLnkaqCoE7zw"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conj_verb(verbo_esp,verbo_ladino):\n",
        "  verbo_phrase = verbo_esp.text\n",
        "  verbo_inf = verbo_esp.lemma\n",
        "  if \"ar\" in verbo_ladino[len(verbo_ladino)-2:len(verbo_ladino)]:\n",
        "    verbo_ladino = ''.join(verbo_ladino.rsplit('ar',1))\n",
        "  if \"er\" in verbo_ladino[len(verbo_ladino)-2:len(verbo_ladino)]:\n",
        "    verbo_ladino = ''.join(verbo_ladino.rsplit('er',1))\n",
        "  if \"ir\" in verbo_ladino[len(verbo_ladino)-2:len(verbo_ladino)]:\n",
        "    verbo_ladino = ''.join(verbo_ladino.rsplit('ir',1))\n",
        "\n",
        "  if \"ando\" in verbo_phrase:\n",
        "    verbo_ladino = verbo_ladino+\"ando\"\n",
        "  elif \"iendo\" in verbo_phrase:\n",
        "    verbo_ladino = verbo_ladino+\"iendo\"\n",
        "  elif \"ado\" in verbo_phrase:\n",
        "    verbo_ladino = verbo_ladino+\"ado\"\n",
        "  elif \"ido\" in verbo_phrase:\n",
        "    verbo_ladino = verbo_ladino+\"ido\"\n",
        "  else:\n",
        "    suff = get_suffix(verbo_phrase,verbo_inf)\n",
        "    verbo_ladino = verbo_ladino+suff\n",
        "  return verbo_ladino"
      ],
      "metadata": {
        "id": "RS0cM1AHdiBF"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conj_adj_sustantivo(word_esp,word_ladino):\n",
        "  if \"Sing\" in word_esp.feats:\n",
        "    suff_word_esp = word_esp.text[len(word_esp.text)-1:]\n",
        "    root_word_lad = word_ladino[:len(word_ladino)-1]\n",
        "    word_ladino = root_word_lad+suff_word_esp\n",
        "    return word_ladino\n",
        "  if \"Plur\" in word_esp.feats:\n",
        "    suff_word_esp = word_esp.text[len(word_esp.text)-2:]\n",
        "    root_word_lad = word_ladino[:len(word_ladino)-1]\n",
        "    word_ladino = root_word_lad+suff_word_esp\n",
        "    return word_ladino"
      ],
      "metadata": {
        "id": "kR3CTWirj8aS"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phrase=\"Hoy me compre dos casas\""
      ],
      "metadata": {
        "id": "e7CKPkh2As8w"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(phrase):\n",
        "  doc = nlp(phrase)\n",
        "  jud_phrase = \"\"\n",
        "  w = \"\"\n",
        "  for sent in doc.sentences:\n",
        "    for word in sent.words:\n",
        "      flag1 = 0\n",
        "      español_word = word.text\n",
        "      español_word = español_word.lower()\n",
        "      español_word = remove_punctuation(español_word)\n",
        "      for d in dic:\n",
        "        if word.lemma == d[\"src\"] or word.text == d[\"src\"]:\n",
        "          ladino_word = d[\"target\"].replace(\" \",\"\").replace(\"\\n\",\"\")\n",
        "          if word.lemma == ladino_word or word.text == ladino_word:\n",
        "            w = español_word\n",
        "            break\n",
        "          if word.upos == \"VERB\":\n",
        "            w = conj_verb(word,ladino_word)\n",
        "          elif word.upos == \"NOUN\" or word.upos == \"ADJ\":\n",
        "            w = conj_adj_sustantivo(word,ladino_word)\n",
        "          else:\n",
        "            w = ladino_word\n",
        "          flag1 = 1\n",
        "          break\n",
        "      if flag1 == 0:\n",
        "          w = judeo_parse(español_word)\n",
        "      jud_phrase += w.replace(\"\\n\",\"\") + \" \"\n",
        "  return jud_phrase.strip()"
      ],
      "metadata": {
        "id": "WKrx2MDC2JSo"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate(phrase)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "s4woYSitO4FK",
        "outputId": "8f6e30b1-3620-4545-c01a-6598327eb1a6"
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'oy me marke dos kazas'"
            ]
          },
          "metadata": {},
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing"
      ],
      "metadata": {
        "id": "G3D2GyA1OJk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_excel(\"una-fraze-al-dia_lad-tur-eng-spa.xlsx\")\n",
        "\n",
        "esp=[]\n",
        "lad_correct = []\n",
        "lad_trans = []\n",
        "\n",
        "for a in df.index:\n",
        "  esp.append(df[\"Español\"][a])\n",
        "  lad_correct.append(df[\"Ladino\"][a])\n",
        "  lad_trans.append(translate(df[\"Español\"][a]))\n",
        "\n",
        "p = {'Español':esp,'Ladino':lad_correct,'Ladino_Trans':lad_trans}\n",
        "\n",
        "df_1 = pd.DataFrame(p)\n",
        "df_1.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "HzGQh988OJSD",
        "outputId": "33530f4f-7884-4c43-a5b5-78c2f68d03c1"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/stanza/models/common/beam.py:86: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  prevK = bestScoresId // numWords\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-124-03ac84b50d4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mesp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Español\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mlad_correct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Ladino\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0mlad_trans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Español\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'Español'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mesp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Ladino'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlad_correct\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Ladino_Trans'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlad_trans\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-122-3481a83625a9>\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(phrase)\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0mespañol_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_punctuation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mespañol_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdic\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"src\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m           \u001b[0mladino_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mladino_word\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file1 = open('dic-judeo-español.txt', 'r', encoding=\"utf-8\")\n",
        "Lines = file1.readlines()\n",
        "dic=[]\n",
        "for l in Lines:\n",
        "  word_esp = l.split(\";\")[0]\n",
        "  word_ladino = l.split(\";\")[1]\n",
        "  if len(word_esp.split(\" \")) == 1 or len(word_esp.split(\",\")) == 1: \n",
        "    doc = nlp(word_esp)\n",
        "    for sent in doc.sentences:\n",
        "      for word in sent.words:\n",
        "        if word.upos == \"NOUN\" and str(word.feats) != \"None\":\n",
        "          if \"Sing\" in word.feats and \"Masc\" in word.feats:\n",
        "            if word_esp[len(word_esp)-1:] == \"o\":\n",
        "              root_word_esp = word_esp[:len(word_esp)-1]\n",
        "              root_word_lad = word_ladino[:len(word_ladino)-2]\n",
        "              word_esp = root_word_esp+\"a\"\n",
        "              word_ladino = root_word_lad+\"a\"\n",
        "              p = word_esp+\";\"+word_ladino\n",
        "              dic.append(p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqOMKKCtsB3h",
        "outputId": "2df542bf-26b1-4618-9bbc-4f92a1090e92"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/stanza/models/common/beam.py:86: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  prevK = bestScoresId // numWords\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using a new POS-Tagging for Spanish"
      ],
      "metadata": {
        "id": "11cVjexmA3a2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "eoGRCxH07_uz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "tagged_sentences = nltk.corpus.conll2002.tagged_sents('esp.train')"
      ],
      "metadata": {
        "id": "uBJSfwIh78_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def features(sentence, index):\n",
        "    \"\"\" sentence: [w1, w2, ...], index: the index of the word \"\"\"\n",
        "    return {\n",
        "        'word': sentence[index],\n",
        "        'is_first': index == 0,\n",
        "        'is_last': index == len(sentence) - 1,\n",
        "        'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n",
        "        'is_all_caps': sentence[index].upper() == sentence[index],\n",
        "        'is_all_lower': sentence[index].lower() == sentence[index],\n",
        "        'prefix-1': sentence[index][0],\n",
        "        'prefix-2': sentence[index][:3],\n",
        "        'prefix-3': sentence[index][:4],\n",
        "        'suffix-1': sentence[index][-1],\n",
        "        'suffix-2': sentence[index][-3:],\n",
        "        'suffix-3': sentence[index][-4:],\n",
        "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
        "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
        "        'has_hyphen': '-' in sentence[index],\n",
        "        'is_numeric': sentence[index].isdigit(),\n",
        "        'capitals_inside': sentence[index][1:].lower() != sentence[index][1:]\n",
        "    }"
      ],
      "metadata": {
        "id": "FiYMkBvI8HjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def untag(tagged_sentence):\n",
        "    return [w for w, t in tagged_sentence]"
      ],
      "metadata": {
        "id": "sLtxUjLC8JBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cutoff = int(.75 * len(tagged_sentences))\n",
        "training_sentences = tagged_sentences[:cutoff]\n",
        "test_sentences = tagged_sentences[cutoff:]\n",
        "\n",
        "print(len(training_sentences))\n",
        "print(len(test_sentences))\n",
        "\n",
        "def transform_to_dataset(tagged_sentences):\n",
        "    X, y = [], []\n",
        "    \n",
        "    for tagged in tagged_sentences:\n",
        "        for index in range(len(tagged)):\n",
        "            X.append(features(untag(tagged),index))\n",
        "            y.append(tagged[index][1])\n",
        "    return X, y\n",
        "\n",
        "X, y = transform_to_dataset(training_sentences)"
      ],
      "metadata": {
        "id": "PuttzAUT8Ocm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "clf = Pipeline([\n",
        "    ('vectorizer', DictVectorizer(sparse=False)),\n",
        "    ('classifier', DecisionTreeClassifier(criterion='entropy'))\n",
        "])\n",
        "\n",
        "clf.fit(X[:20000], y[:20000])\n",
        "\n",
        "print('Training completed')"
      ],
      "metadata": {
        "id": "4u-DxJmt8Rqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pos_tag(sentence):\n",
        "    tags = clf.predict([features(sentence, index) for index in range(len(sentence))])\n",
        "    return list(zip(sentence, tags))"
      ],
      "metadata": {
        "id": "6-AuktJZ8Vlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "print (pos_tag(word_tokenize('El canciller de Alemania llegó el lunes a Rusia')))"
      ],
      "metadata": {
        "id": "e2guYLok8YM1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}